"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API - ë¶„ê¸°ë³„ ë¶„í•  ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
ì‘ì„±ì¼: 2025-10-24
"""

import os
import sys
import time
import requests
import pandas as pd
from math import ceil
from datetime import datetime, timedelta
import argparse
import yaml
from pathlib import Path
from dotenv import load_dotenv
import logging

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) ì„¤ì • ë¡œë“œ
def load_config(config_path: str = "config.yaml") -> dict:
    """YAML ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    except FileNotFoundError:
        print(f"âš ï¸ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        print("ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return get_default_config()
    except yaml.YAMLError as e:
        print(f"âš ï¸ YAML íŒŒì‹± ì˜¤ë¥˜: {e}")
        print("ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return get_default_config()

def get_default_config() -> dict:
    """ê¸°ë³¸ ì„¤ì • ë°˜í™˜"""
    return {
        'keywords': ["ì²­ë„í˜ì‹ ì„¼í„°", "ê²½ë¶ì‹œë¯¼ì¬ë‹¨", "ë¡œì»¬ì„íŒ©íŠ¸ë©", "ê²½ë¶ì§€ì†ê°€ëŠ¥ìº í”„", "ì²­ë„êµ°"],
        'collection': {
            'start_year': 2022,
            'display_per_page': 100,
            'max_items_per_query': 1000,
            'api_call_delay': 0.1,
            'request_timeout': 10
        },
        'output': {
            'parts_dir': 'output_parts',
            'merged_dir': 'output',
            'merged_filename': 'news_merged.csv',
            'encoding': 'utf-8-sig'
        },
        'logging': {
            'dir': 'logs',
            'level': 'INFO',
            'format': '%(asctime)s - %(levelname)s - %(message)s'
        },
        'filtering': {
            'exact_phrase_match': True,
            'remove_duplicates': True,
            'duplicate_check_column': 'url'
        },
        'api': {
            'search_endpoint': 'https://openapi.naver.com/v1/search',
            'sort': 'date',
            'retry_count': 3,
            'retry_delay': 1.0
        }
    }

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì„¤ì • ë¡œë“œ
CONFIG = load_config()

# ì¸ì¦ ì •ë³´
CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

if not CLIENT_ID or not CLIENT_SECRET:
    print("âŒ ì˜¤ë¥˜: ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”:")
    print("  1. .env íŒŒì¼ì„ ìƒì„±í•˜ê³  NAVER_CLIENT_IDì™€ NAVER_CLIENT_SECRETì„ ì„¤ì •")
    print("  2. í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •: export NAVER_CLIENT_ID=your_id")
    sys.exit(1)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) ë¡œê¹… ì„¤ì •
def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    log_config = CONFIG.get('logging', {})
    log_dir = log_config.get('dir', 'logs')
    log_level = log_config.get('level', 'INFO')
    log_format = log_config.get('format', '%(asctime)s - %(levelname)s - %(message)s')
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(log_dir).mkdir(exist_ok=True)
    
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    level = getattr(logging, log_level.upper(), logging.INFO)
    
    # ë¡œê±° ì„¤ì •
    logging.basicConfig(
        level=level,
        format=log_format,
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(
                os.path.join(log_dir, f"collection_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"),
                encoding='utf-8'
            )
        ]
    )
    
    return logging.getLogger(__name__)

logger = setup_logging()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) Search API ê³µí†µ í˜¸ì¶œ í•¨ìˆ˜
def naver_search(endpoint: str, query: str, display: int = None, start: int = 1, sort: str = None) -> dict:
    """ë„¤ì´ë²„ ê²€ìƒ‰ API í˜¸ì¶œ"""
    api_config = CONFIG.get('api', {})
    collection_config = CONFIG.get('collection', {})
    
    if display is None:
        display = collection_config.get('display_per_page', 100)
    if sort is None:
        sort = api_config.get('sort', 'date')
    
    base_url = api_config.get('search_endpoint', 'https://openapi.naver.com/v1/search')
    url = f"{base_url}/{endpoint}.json"
    
    headers = {
        "X-Naver-Client-Id": CLIENT_ID,
        "X-Naver-Client-Secret": CLIENT_SECRET,
    }
    params = {
        "query": query, 
        "display": display, 
        "start": start,
        "sort": sort
    }
    
    timeout = collection_config.get('request_timeout', 10)
    retry_count = api_config.get('retry_count', 3)
    retry_delay = api_config.get('retry_delay', 1.0)
    
    for attempt in range(retry_count):
        try:
            res = requests.get(url, headers=headers, params=params, timeout=timeout)
            res.raise_for_status()
            return res.json()
        except requests.exceptions.RequestException as e:
            if attempt < retry_count - 1:
                logger.warning(f"API í˜¸ì¶œ ì‹¤íŒ¨ (ì¬ì‹œë„ {attempt + 1}/{retry_count}): {e}")
                time.sleep(retry_delay)
            else:
                logger.error(f"API í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {e}")
                return {"total": 0, "items": []}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) ë¶„ê¸°ë³„ ë‚ ì§œ ë²”ìœ„ ìƒì„±
def generate_quarterly_ranges(start_year: int, end_year: int = None) -> list:
    """ë¶„ê¸°ë³„ ë‚ ì§œ ë²”ìœ„ ìƒì„±"""
    if end_year is None:
        end_year = datetime.now().year
    
    quarters = []
    quarter_months = [
        ("Q1", 1, 3),
        ("Q2", 4, 6),
        ("Q3", 7, 9),
        ("Q4", 10, 12)
    ]
    
    for year in range(start_year, end_year + 1):
        for q_name, start_month, end_month in quarter_months:
            start_date = datetime(year, start_month, 1)
            
            if end_month == 12:
                end_date = datetime(year, 12, 31)
            else:
                end_date = datetime(year, end_month + 1, 1) - timedelta(days=1)
            
            if start_date > datetime.now():
                break
            
            if end_date > datetime.now():
                end_date = datetime.now()
            
            quarter_label = f"{year}_{q_name}"
            quarters.append((
                quarter_label,
                start_date.strftime("%Y-%m-%d"),
                end_date.strftime("%Y-%m-%d")
            ))
    
    return quarters

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5) í˜ì´ì§• ìˆ˜ì§‘
def fetch_news_in_quarter(keyword: str, quarter_name: str, start_date: str, end_date: str) -> pd.DataFrame:
    """íŠ¹ì • ë¶„ê¸°ì˜ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘"""
    collection_config = CONFIG.get('collection', {})
    display = collection_config.get('display_per_page', 100)
    max_items = collection_config.get('max_items_per_query', 1000)
    api_delay = collection_config.get('api_call_delay', 0.1)
    
    logger.info(f"[{quarter_name}] {start_date} ~ {end_date}")
    
    first = naver_search("news", keyword, display=display, start=1)
    total_available = first.get("total", 0)
    total_to_fetch = min(total_available, max_items)

    items = first.get("items", [])
    
    if total_to_fetch == 0:
        logger.info(f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return pd.DataFrame()
    
    pages = ceil(total_to_fetch / display)

    for page in range(2, pages + 1):
        start = (page - 1) * display + 1
        time.sleep(api_delay)
        
        js = naver_search("news", keyword, display=display, start=start)
        batch = js.get("items", [])
        
        if not batch:
            break
        
        items.extend(batch)
        
        if len(items) >= max_items:
            items = items[:max_items]
            break

    df = pd.DataFrame(items)
    if not df.empty:
        df = df[["title", "link", "originallink", "description", "pubDate"]].rename(columns={
            "link": "url", 
            "originallink": "source_url", 
            "pubDate": "date"
        })
        
        df["title"] = df["title"].str.replace(r"</?b>", "", regex=True)
        df["description"] = df["description"].str.replace(r"</?b>", "", regex=True)
        
        df = filter_by_date_range(df, start_date, end_date)
        logger.info(f"ìˆ˜ì§‘ ì™„ë£Œ: {len(df)}ê±´")
    else:
        logger.info(f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
    
    return df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6) ë‚ ì§œ ë²”ìœ„ í•„í„°ë§
def filter_by_date_range(df: pd.DataFrame, start_date: str, end_date: str) -> pd.DataFrame:
    """ë‚ ì§œ ë²”ìœ„ë¡œ í•„í„°ë§"""
    if df.empty:
        return df
    
    df["parsed_date"] = pd.to_datetime(df["date"], format="%a, %d %b %Y %H:%M:%S %z", errors="coerce")
    
    start = pd.to_datetime(start_date)
    end = pd.to_datetime(end_date) + timedelta(days=1)
    
    mask = (df["parsed_date"] >= start) & (df["parsed_date"] < end)
    filtered_df = df[mask].copy()
    
    if "parsed_date" in filtered_df.columns:
        filtered_df = filtered_df.drop(columns=["parsed_date"])
    
    return filtered_df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7) ì •í™• ì—°ì†ì–´êµ¬ í•„í„°ë§
def filter_exact_phrase(df: pd.DataFrame, phrase: str) -> pd.DataFrame:
    """ì œëª© ë˜ëŠ” ì„¤ëª…ì— ì •í™•í•œ ë¬¸êµ¬ê°€ í¬í•¨ëœ í•­ëª©ë§Œ í•„í„°ë§"""
    if df.empty:
        return df
    
    filtering_config = CONFIG.get('filtering', {})
    if not filtering_config.get('exact_phrase_match', True):
        return df
    
    mask_title = df["title"].str.contains(phrase, regex=False, na=False)
    mask_desc = df["description"].str.contains(phrase, regex=False, na=False)
    return df[mask_title | mask_desc]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8) ë‹¨ì¼ í‚¤ì›Œë“œ, ë‹¨ì¼ ë¶„ê¸° ìˆ˜ì§‘
def collect_single_keyword_quarter(keyword: str, quarter_name: str, start_date: str, 
                                   end_date: str, output_dir: str = None) -> str:
    """ë‹¨ì¼ í‚¤ì›Œë“œì™€ ë‹¨ì¼ ë¶„ê¸° ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥"""
    if output_dir is None:
        output_dir = CONFIG.get('output', {}).get('parts_dir', 'output_parts')
    
    logger.info(f"=" * 60)
    logger.info(f"í‚¤ì›Œë“œ: '{keyword}' | ë¶„ê¸°: {quarter_name}")
    logger.info(f"=" * 60)
    
    # ë°ì´í„° ìˆ˜ì§‘
    df = fetch_news_in_quarter(keyword, quarter_name, start_date, end_date)
    
    if df.empty:
        logger.warning(f"ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ")
        return None
    
    # ì •í™• ë¬¸êµ¬ í•„í„°ë§
    df_exact = filter_exact_phrase(df, keyword)
    logger.info(f"ì •í™• ë¬¸êµ¬ ë§¤ì¹­: {len(df_exact)}ê±´")
    
    if df_exact.empty:
        logger.warning(f"í•„í„°ë§ í›„ ë°ì´í„° ì—†ìŒ")
        return None
    
    # ë¶„ê¸° ì •ë³´ ì¶”ê°€
    df_exact["quarter"] = quarter_name
    df_exact["keyword"] = keyword
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(output_dir).mkdir(exist_ok=True)
    
    # íŒŒì¼ëª…
    filename = f"{keyword}_{quarter_name}.csv"
    filepath = os.path.join(output_dir, filename)
    
    # CSV ì €ì¥
    encoding = CONFIG.get('output', {}).get('encoding', 'utf-8-sig')
    df_exact.to_csv(filepath, index=False, encoding=encoding)
    logger.info(f"ì €ì¥ ì™„ë£Œ: {filepath} ({len(df_exact)}ê±´)")
    
    return filepath

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 9) ì „ì²´ í‚¤ì›Œë“œ, ì „ì²´ ë¶„ê¸° ìˆ˜ì§‘
def collect_all_keywords_all_quarters(keywords: list = None, start_year: int = None, 
                                      output_dir: str = None):
    """ëª¨ë“  í‚¤ì›Œë“œ, ëª¨ë“  ë¶„ê¸° ìˆ˜ì§‘"""
    if keywords is None:
        keywords = CONFIG.get('keywords', [])
    if start_year is None:
        start_year = CONFIG.get('collection', {}).get('start_year', 2022)
    if output_dir is None:
        output_dir = CONFIG.get('output', {}).get('parts_dir', 'output_parts')
    
    logger.info("=" * 60)
    logger.info("ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API - ì „ì²´ ë¶„í•  ìˆ˜ì§‘")
    logger.info("=" * 60)
    
    # ë¶„ê¸° ëª©ë¡ ìƒì„±
    quarters = generate_quarterly_ranges(start_year)
    
    logger.info(f"ë¶„ê¸° ê°œìˆ˜: {len(quarters)}")
    logger.info(f"í‚¤ì›Œë“œ ê°œìˆ˜: {len(keywords)}")
    logger.info(f"ì´ ì‘ì—… ê°œìˆ˜: {len(keywords) * len(quarters)}")
    
    # ê° í‚¤ì›Œë“œ, ê° ë¶„ê¸°ë³„ë¡œ ìˆ˜ì§‘
    saved_files = []
    total_tasks = len(keywords) * len(quarters)
    current_task = 0
    
    api_delay = CONFIG.get('collection', {}).get('api_call_delay', 0.1)
    
    for keyword in keywords:
        for quarter_name, start_date, end_date in quarters:
            current_task += 1
            logger.info(f"\n[{current_task}/{total_tasks}] ì‘ì—… ì§„í–‰ ì¤‘...")
            
            try:
                filepath = collect_single_keyword_quarter(
                    keyword, quarter_name, start_date, end_date, output_dir
                )
                
                if filepath:
                    saved_files.append(filepath)
            
            except Exception as e:
                logger.error(f"{keyword}_{quarter_name} - ì˜¤ë¥˜: {e}")
            
            # API í˜¸ì¶œ ì œí•œ ê³ ë ¤
            time.sleep(api_delay * 5)
    
    logger.info(f"\n{'='*60}")
    logger.info(f"ì „ì²´ ì‘ì—… ì™„ë£Œ!")
    logger.info(f"ì €ì¥ëœ íŒŒì¼ ê°œìˆ˜: {len(saved_files)}")
    logger.info(f"ì €ì¥ ìœ„ì¹˜: {output_dir}/")
    logger.info(f"{'='*60}")
    
    return saved_files

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 10) ë¶„í• ëœ íŒŒì¼ ë³‘í•©
def merge_all_parts(input_dir: str = None, output_file: str = None):
    """ë¶„í• ëœ CSV íŒŒì¼ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•©"""
    if input_dir is None:
        input_dir = CONFIG.get('output', {}).get('parts_dir', 'output_parts')
    if output_file is None:
        output_config = CONFIG.get('output', {})
        output_dir = output_config.get('merged_dir', 'output')
        filename = output_config.get('merged_filename', 'news_merged.csv')
        output_file = os.path.join(output_dir, filename)
    
    logger.info("\n" + "=" * 60)
    logger.info("ë¶„í•  íŒŒì¼ ë³‘í•© ì‹œì‘")
    logger.info("=" * 60)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(os.path.dirname(output_file)).mkdir(exist_ok=True)
    
    # CSV íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]
    
    if not csv_files:
        logger.error(f"'{input_dir}' ë””ë ‰í† ë¦¬ì— CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    logger.info(f"ë°œê²¬ëœ íŒŒì¼ ê°œìˆ˜: {len(csv_files)}")
    
    # ëª¨ë“  íŒŒì¼ ì½ì–´ì„œ ë³‘í•©
    encoding = CONFIG.get('output', {}).get('encoding', 'utf-8-sig')
    all_dfs = []
    
    for i, filename in enumerate(csv_files, 1):
        filepath = os.path.join(input_dir, filename)
        try:
            df = pd.read_csv(filepath, encoding=encoding)
            all_dfs.append(df)
            if i % 10 == 0:
                logger.info(f"{i}/{len(csv_files)} íŒŒì¼ ì½ê¸° ì™„ë£Œ...")
        except Exception as e:
            logger.warning(f"{filename} ì½ê¸° ì‹¤íŒ¨: {e}")
    
    if not all_dfs:
        logger.error("ì½ì„ ìˆ˜ ìˆëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë³‘í•©
    df_merged = pd.concat(all_dfs, ignore_index=True)
    logger.info(f"\në³‘í•© ì „ ì´ ë ˆì½”ë“œ: {len(df_merged):,}ê±´")
    
    # ì¤‘ë³µ ì œê±°
    filtering_config = CONFIG.get('filtering', {})
    if filtering_config.get('remove_duplicates', True):
        initial_count = len(df_merged)
        dup_column = filtering_config.get('duplicate_check_column', 'url')
        df_merged = df_merged.drop_duplicates(subset=[dup_column]).reset_index(drop=True)
        duplicates_removed = initial_count - len(df_merged)
        
        logger.info(f"ì¤‘ë³µ ì œê±°: {duplicates_removed:,}ê±´")
    
    logger.info(f"ìµœì¢… ë ˆì½”ë“œ: {len(df_merged):,}ê±´")
    
    # ì €ì¥
    df_merged.to_csv(output_file, index=False, encoding=encoding)
    logger.info(f"\në³‘í•© ì™„ë£Œ: {output_file}")
    
    # í†µê³„ ì¶œë ¥
    if "keyword" in df_merged.columns:
        logger.info(f"\ní‚¤ì›Œë“œë³„ í†µê³„:")
        keyword_counts = df_merged["keyword"].value_counts()
        for keyword, count in keyword_counts.items():
            logger.info(f"  - {keyword}: {count:,}ê±´")
    
    if "quarter" in df_merged.columns:
        logger.info(f"\në¶„ê¸°ë³„ í†µê³„ (ìƒìœ„ 10ê°œ):")
        quarter_counts = df_merged["quarter"].value_counts().sort_index().tail(10)
        for quarter, count in quarter_counts.items():
            logger.info(f"  - {quarter}: {count:,}ê±´")
    
    logger.info("=" * 60)
    
    return output_file

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 11) ë©”ì¸ í•¨ìˆ˜
def main():
    parser = argparse.ArgumentParser(description="ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API - ë¶„í•  ì‘ì—…")
    parser.add_argument("--mode", choices=["single", "all", "merge"], default="all",
                       help="ì‹¤í–‰ ëª¨ë“œ: single(ë‹¨ì¼ ì‘ì—…), all(ì „ì²´ ì‹¤í–‰), merge(ë³‘í•©)")
    parser.add_argument("--keyword", type=str, help="ê²€ìƒ‰ í‚¤ì›Œë“œ (single ëª¨ë“œ)")
    parser.add_argument("--quarter", type=str, help="ë¶„ê¸° (ì˜ˆ: 2022_Q1) (single ëª¨ë“œ)")
    parser.add_argument("--start-date", type=str, help="ì‹œì‘ì¼ (YYYY-MM-DD) (single ëª¨ë“œ)")
    parser.add_argument("--end-date", type=str, help="ì¢…ë£Œì¼ (YYYY-MM-DD) (single ëª¨ë“œ)")
    parser.add_argument("--output-dir", type=str, help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--start-year", type=int, help="ì‹œì‘ ì—°ë„ (all ëª¨ë“œ)")
    parser.add_argument("--config", type=str, default="config.yaml", help="ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    
    args = parser.parse_args()
    
    # ì„¤ì • íŒŒì¼ ì¬ë¡œë“œ (ì»¤ë§¨ë“œë¼ì¸ì—ì„œ ì§€ì •í•œ ê²½ìš°)
    if args.config != "config.yaml":
        global CONFIG
        CONFIG = load_config(args.config)
    
    keywords = CONFIG.get('keywords', [])
    
    if args.mode == "single":
        # ë‹¨ì¼ ì‘ì—… ì‹¤í–‰
        if not all([args.keyword, args.quarter, args.start_date, args.end_date]):
            logger.error("single ëª¨ë“œëŠ” --keyword, --quarter, --start-date, --end-dateê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        collect_single_keyword_quarter(
            args.keyword, 
            args.quarter, 
            args.start_date, 
            args.end_date,
            args.output_dir
        )
    
    elif args.mode == "all":
        # ì „ì²´ ì‹¤í–‰
        start_year = args.start_year or CONFIG.get('collection', {}).get('start_year', 2022)
        collect_all_keywords_all_quarters(
            keywords,
            start_year,
            args.output_dir
        )
    
    elif args.mode == "merge":
        # ë³‘í•©
        merge_all_parts(args.output_dir)

if __name__ == "__main__":
    if len(sys.argv) == 1:
        # ëª…ë ¹ì¤„ ì¸ì ì—†ì´ ì‹¤í–‰í•˜ë©´ ì „ì²´ ìë™ ì‹¤í–‰
        logger.info("ğŸš€ ìë™ ì‹¤í–‰ ëª¨ë“œ: ì „ì²´ ìˆ˜ì§‘ + ë³‘í•©")
        collect_all_keywords_all_quarters()
        merge_all_parts()
    else:
        main()
